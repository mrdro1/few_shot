{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "from razdel import sentenize, tokenize\n",
    "from bpe import Encoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_(text):\n",
    "    tokens = tokenize(text)\n",
    "    return [_.text for _ in tokens]\n",
    "    \n",
    "def remove_tags(text):\n",
    "    text  = re.sub(\"<.*?>\", \"\", text).replace('\\n', ' ').replace('&nbsp;', ' ').replace('&mdash;', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lenta.csv')\n",
    "df['title'] = df['title'].apply(lambda x: str(x).replace('\\xa0', ' '))\n",
    "df['full_text'] = df['title'].map(lambda x: str(x) + ' ') + \\\n",
    "                df['text'].map(lambda x: str(x).replace('\\n', ' '))\n",
    "df[['full_text', 'tags']].to_csv('data/lenta_text.txt', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "encoder = Encoder(VOCAB_SIZE, word_tokenizer=tokenize_) \n",
    "encoder = encoder.load('bpe_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['австрия',\n",
       "  'не',\n",
       "  'представила',\n",
       "  'доказательств',\n",
       "  'вины',\n",
       "  'российских',\n",
       "  'биатлонистов',\n",
       "  '__sow',\n",
       "  'ав',\n",
       "  'ст',\n",
       "  'ри',\n",
       "  'йс',\n",
       "  'ки',\n",
       "  'е',\n",
       "  '__eow',\n",
       "  'правоохранительные',\n",
       "  'органы',\n",
       "  'не',\n",
       "  'представили',\n",
       "  'доказательств',\n",
       "  'нарушения',\n",
       "  'российскими',\n",
       "  '__sow',\n",
       "  'би',\n",
       "  'ат',\n",
       "  'ло',\n",
       "  'ни',\n",
       "  'ст',\n",
       "  'ам',\n",
       "  'и'],\n",
       " 'Зимние виды')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.tokenize(df['full_text'].values[1]))[:30], df['tags'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df['tags'].value_counts()\n",
    "tags = tags[tags>100]\n",
    "tags = tags[tags<34_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data/lenta_text.txt', 'r', encoding='UTF-8')\n",
    "len([1 for _ in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60\n",
    "\n",
    "def gen_train(params):\n",
    "    file = open('data/lenta_text.txt', 'r', encoding='UTF-8')\n",
    "    for k, line in enumerate(file):\n",
    "        if k>params['max_texts']:\n",
    "            break\n",
    "        try:\n",
    "            idxs = list(encoder.transform([line.strip().split('|')[0]]))[0]\n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "        len_idxs = len(idxs)\n",
    "        if len_idxs<maxlen:\n",
    "            idxs = idxs + [0]*(maxlen-len_idxs)\n",
    "        yield [0] + idxs[:maxlen-1], maxlen, maxlen, idxs[:maxlen]\n",
    "\n",
    "\n",
    "def parser(input_seq, len_seq, max_len, output_seq):\n",
    "    return {'input_seq':input_seq, 'len_seq': len_seq, 'max_len': max_len}, output_seq\n",
    "\n",
    "def input_fn_train(params, is_training):\n",
    "    dataset = tf.data.Dataset.from_generator(lambda: gen_train(params),\n",
    "                                             (tf.int64,tf.int64,tf.int64,tf.int64),\n",
    "                                              output_shapes=(tf.TensorShape([None]), \n",
    "                                                             tf.TensorShape([]),\n",
    "                                                             tf.TensorShape([]),\n",
    "                                                             tf.TensorShape([None])))\n",
    "    if is_training:\n",
    "        dataset = dataset.repeat(count=params['num_epochs'])\n",
    "        dataset = dataset.shuffle(10000)\n",
    "\n",
    "    dataset = dataset.batch(params['batch_size'])\n",
    "    dataset = dataset.map(parser)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "def build_model(features, params, is_training):\n",
    "    \n",
    "    # Embedding matrix\n",
    "    bpe_emb_matrix = tf.get_variable('bpe_embedding_matrix',\n",
    "                                 shape=[params['vocab_size'], params['emb_size']],\n",
    "                                 dtype=tf.float32)\n",
    "\n",
    "    def encode(x):\n",
    "        # embedding\n",
    "        embs = tf.nn.embedding_lookup(bpe_emb_matrix, x['input_seq'])\n",
    "        # dropout\n",
    "        dropout_emb = tf.layers.dropout(inputs=embs, \n",
    "                                        rate=0.1, \n",
    "                                        training=is_training)\n",
    "        # lstm\n",
    "        lstm_cell_1 = tf.nn.rnn_cell.GRUCell(250)\n",
    "        outputs, final_states = tf.nn.dynamic_rnn(\n",
    "            lstm_cell_1, dropout_emb, sequence_length=x['len_seq'], dtype=tf.float32)\n",
    "\n",
    "        # for futher clf\n",
    "        max_pool = tf.reduce_max(input_tensor=outputs, axis=1)\n",
    "        mean_pool = tf.reduce_mean(input_tensor=outputs, axis=1)\n",
    "        concat_pools = tf.concat((mean_pool, max_pool, final_states),1)\n",
    "        \n",
    "        # for lm\n",
    "        dropout_lstm = tf.layers.dropout(inputs=outputs, \n",
    "                                        rate=0.1, \n",
    "                                        training=is_training)\n",
    "        logits = tf.layers.dense(dropout_lstm, params['vocab_size'])\n",
    "        \n",
    "        \n",
    "        return logits, concat_pools\n",
    "\n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "        logits, concat_pools = encode(features)\n",
    "    \n",
    "    return logits, concat_pools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "def triangular_learning_rate(global_step, max_steps,\n",
    "                             max_lr=0.001,\n",
    "                             ratio=32.,\n",
    "                             cut_frac=0.1,\n",
    "                            name=None):\n",
    "\n",
    "    if global_step is None:\n",
    "        raise ValueError(\"global_step is required for cyclic_learning_rate.\")\n",
    "    with ops.name_scope(name, \"TriangularLearningRate\",\n",
    "                      [global_step]) as name:\n",
    "        max_lr = ops.convert_to_tensor(max_lr)\n",
    "        dtype = max_lr.dtype\n",
    "        global_step = math_ops.cast(global_step, dtype)\n",
    "        ratio = math_ops.cast(ratio, dtype)\n",
    "        cut_frac = math_ops.cast(cut_frac, dtype)\n",
    "        max_steps = math_ops.cast(max_steps, dtype)\n",
    "        \n",
    "        def triangular_lr():\n",
    "            cut = math_ops.floor(math_ops.multiply(max_steps, cut_frac))\n",
    "            # 1\n",
    "            p1 = math_ops.divide(global_step, cut)\n",
    "            # 2\n",
    "            up = math_ops.subtract(global_step, cut)\n",
    "            right = math_ops.subtract(math_ops.divide(1., cut_frac), 1.)\n",
    "            down = math_ops.multiply(cut, right)\n",
    "            p2 = math_ops.subtract(1., math_ops.divide(up, down))\n",
    "            #\n",
    "            p = tf.cond(tf.math.greater(global_step, cut), lambda: p2, lambda: p1)\n",
    "            up = math_ops.add(1., math_ops.multiply(p, math_ops.subtract(ratio, 1.)))\n",
    "            lr = math_ops.multiply(max_lr, math_ops.divide(up, ratio))\n",
    "            return lr\n",
    "        tlr = triangular_lr()\n",
    "        return tlr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'ckpt/ulmfit_st2_experiment_15', '_tf_random_seed': 123, '_save_summary_steps': 5, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DC1F3E13C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\mrdro\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\mrdro\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-7-5fced8459b41>:15: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From C:\\Users\\mrdro\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-7-5fced8459b41>:17: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-5fced8459b41>:19: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\mrdro\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-5fced8459b41>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-60ad671175dc>:12: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:loss = 9.0348015, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.28866\n",
      "INFO:tensorflow:loss = 9.190635, step = 101 (77.604 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.31607\n",
      "INFO:tensorflow:loss = 9.130739, step = 201 (75.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.28565\n",
      "INFO:tensorflow:loss = 9.13274, step = 301 (77.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.26685\n",
      "INFO:tensorflow:loss = 9.195552, step = 401 (78.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.32475\n",
      "INFO:tensorflow:loss = 9.119773, step = 501 (75.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34535\n",
      "INFO:tensorflow:loss = 9.140899, step = 601 (74.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34147\n",
      "INFO:tensorflow:loss = 8.933986, step = 701 (74.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33737\n",
      "INFO:tensorflow:loss = 8.983943, step = 801 (74.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34205\n",
      "INFO:tensorflow:loss = 8.875356, step = 901 (74.512 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.30174\n",
      "INFO:tensorflow:loss = 8.92497, step = 1001 (76.820 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34472\n",
      "INFO:tensorflow:loss = 9.057649, step = 1101 (74.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33737\n",
      "INFO:tensorflow:loss = 9.02971, step = 1201 (74.776 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.28744\n",
      "INFO:tensorflow:loss = 8.908083, step = 1301 (77.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.26114\n",
      "INFO:tensorflow:loss = 8.910152, step = 1401 (79.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.23518\n",
      "INFO:tensorflow:loss = 9.038274, step = 1501 (80.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.23368\n",
      "INFO:tensorflow:loss = 9.001358, step = 1601 (81.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.23324\n",
      "INFO:tensorflow:loss = 8.933598, step = 1701 (81.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.23487\n",
      "INFO:tensorflow:loss = 9.021352, step = 1801 (80.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.16775\n",
      "INFO:tensorflow:loss = 8.893702, step = 1901 (85.633 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.21575\n",
      "INFO:tensorflow:loss = 9.136729, step = 2001 (82.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.28283\n",
      "INFO:tensorflow:loss = 8.864014, step = 2101 (77.950 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.29624\n",
      "INFO:tensorflow:loss = 8.899296, step = 2201 (77.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.23513\n",
      "INFO:tensorflow:loss = 8.8300495, step = 2301 (80.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.22521\n",
      "INFO:tensorflow:loss = 8.89836, step = 2401 (81.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.24503\n",
      "INFO:tensorflow:loss = 9.006032, step = 2501 (80.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.23288\n",
      "INFO:tensorflow:loss = 8.98038, step = 2601 (81.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.27942\n",
      "INFO:tensorflow:loss = 8.977387, step = 2701 (78.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.28145\n",
      "INFO:tensorflow:loss = 9.085527, step = 2801 (78.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.2591\n",
      "INFO:tensorflow:loss = 9.085479, step = 2901 (79.423 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.21769\n",
      "INFO:tensorflow:loss = 8.851968, step = 3001 (82.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.29294\n",
      "INFO:tensorflow:loss = 8.614227, step = 3101 (77.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.32931\n",
      "INFO:tensorflow:loss = 8.818222, step = 3201 (75.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34314\n",
      "INFO:tensorflow:loss = 8.972207, step = 3301 (74.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34277\n",
      "INFO:tensorflow:loss = 9.035878, step = 3401 (74.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34619\n",
      "INFO:tensorflow:loss = 8.78187, step = 3501 (74.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34027\n",
      "INFO:tensorflow:loss = 8.77128, step = 3601 (74.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 8.946537, step = 3701 (74.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34542\n",
      "INFO:tensorflow:loss = 8.87369, step = 3801 (74.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34398\n",
      "INFO:tensorflow:loss = 8.96726, step = 3901 (74.406 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.29752\n",
      "INFO:tensorflow:loss = 8.997647, step = 4001 (77.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33828\n",
      "INFO:tensorflow:loss = 9.042837, step = 4101 (74.723 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33927\n",
      "INFO:tensorflow:loss = 9.0319805, step = 4201 (74.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33938\n",
      "INFO:tensorflow:loss = 8.885153, step = 4301 (74.661 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33831\n",
      "INFO:tensorflow:loss = 8.963545, step = 4401 (74.722 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.3432\n",
      "INFO:tensorflow:loss = 8.836092, step = 4501 (74.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34304\n",
      "INFO:tensorflow:loss = 8.914647, step = 4601 (74.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34721\n",
      "INFO:tensorflow:loss = 8.718817, step = 4701 (74.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.32578\n",
      "INFO:tensorflow:loss = 8.784917, step = 4801 (75.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.32738\n",
      "INFO:tensorflow:loss = 8.849475, step = 4901 (75.377 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "WARNING:tensorflow:From C:\\Users\\mrdro\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:global_step/sec: 1.29635\n",
      "INFO:tensorflow:loss = 8.810041, step = 5001 (77.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33346\n",
      "INFO:tensorflow:loss = 8.758281, step = 5101 (74.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.3351\n",
      "INFO:tensorflow:loss = 8.942015, step = 5201 (74.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33753\n",
      "INFO:tensorflow:loss = 8.874932, step = 5301 (74.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34045\n",
      "INFO:tensorflow:loss = 8.764156, step = 5401 (74.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34074\n",
      "INFO:tensorflow:loss = 8.876874, step = 5501 (74.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33888\n",
      "INFO:tensorflow:loss = 8.749757, step = 5601 (74.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.33539\n",
      "INFO:tensorflow:loss = 8.749122, step = 5701 (74.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.35193\n",
      "INFO:tensorflow:loss = 8.738276, step = 5801 (73.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.31566\n",
      "INFO:tensorflow:loss = 8.9416895, step = 5901 (76.008 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.30112\n",
      "INFO:tensorflow:loss = 8.889294, step = 6001 (76.856 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34955\n",
      "INFO:tensorflow:loss = 8.694424, step = 6101 (74.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.34959\n",
      "INFO:tensorflow:loss = 8.690234, step = 6201 (74.097 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6252 into ckpt/ulmfit_st2_experiment_15\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 9.156418.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1dc1f3e1208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        logits, concat_pools = build_model(features, params, is_training)\n",
    "     \n",
    "    # TRANSFER LEARNING\n",
    "    keys = ['model/encoder']\n",
    "    tf.train.init_from_checkpoint('ckpt/ulmfit_experiment_10', {k+'/': k+'/' for k in keys})\n",
    "    \n",
    "    weight_mask = tf.to_float(tf.sequence_mask(features['max_len']))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits, labels, weight_mask)\n",
    "    \n",
    "    # Learning rate\n",
    "    global_step = tf.train.get_global_step()\n",
    "    lr = triangular_learning_rate(global_step, params['max_step'])\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    \n",
    "    \n",
    "    lr3 = lr\n",
    "    lr2 = math_ops.divide(lr3, 2.6)\n",
    "    lr1 = math_ops.divide(lr2, 2.6)\n",
    "    \n",
    "    # discr finetune   \n",
    "    var_list_1 = [tf.trainable_variables()[0]]\n",
    "    var_list_2 = tf.trainable_variables()[1:-2]\n",
    "    var_list_3 = tf.trainable_variables()[-2:]\n",
    "    opt1 = tf.train.AdadeltaOptimizer(learning_rate=lr1)\n",
    "    opt2 = tf.train.AdadeltaOptimizer(learning_rate=lr2)\n",
    "    opt3 = tf.train.AdadeltaOptimizer(learning_rate=lr3)\n",
    "    \n",
    "    grads = tf.gradients(loss, var_list_1 + var_list_2 + var_list_3)\n",
    "    grads1 = grads[:len(var_list_1)]\n",
    "    grads2 = grads[len(var_list_1):len(var_list_1)+len(var_list_2)]\n",
    "    grads3 = grads[len(var_list_1)+len(var_list_2):]\n",
    "    train_op1 = opt1.apply_gradients(zip(grads1, var_list_1))\n",
    "    train_op2 = opt2.apply_gradients(zip(grads2, var_list_2))\n",
    "    train_op3 = opt2.apply_gradients(zip(grads3, var_list_3))\n",
    "    \n",
    "    update_global_step = tf.assign(global_step, global_step + 1, name = 'update_global_step')\n",
    "\n",
    "    \n",
    "    train_op = tf.group(train_op1, train_op2, train_op3, update_global_step)\n",
    "\n",
    "    \n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "max_texts = 100_000\n",
    "bs = 16\n",
    "ep = 1\n",
    "model_params = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'emb_size': 100,\n",
    "    'batch_size': bs,\n",
    "    'max_texts': max_texts,\n",
    "    'num_epochs': ep,\n",
    "    'max_step': ep*max_texts//bs + 1\n",
    "}\n",
    "model_dir = 'ckpt/ulmfit_st2_experiment_15'\n",
    "\n",
    "\n",
    "config = tf.estimator.RunConfig(tf_random_seed=123,\n",
    "                                model_dir=model_dir,\n",
    "                                save_summary_steps=5,\n",
    "                               save_checkpoints_steps=1000)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn,\n",
    "                                   params=model_params,\n",
    "                                   config=config)\n",
    "estimator.train(input_fn=lambda: input_fn_train(model_params, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
